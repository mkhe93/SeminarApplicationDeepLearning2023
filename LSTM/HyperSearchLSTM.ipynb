{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Hyperparameter search for LSTM"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIRECTORY = Path(os.path.abspath('')).resolve().parents[0]\n",
    "sys.path.extend([str(PROJECT_DIRECTORY)])\n",
    "\n",
    "DATA_DIRECTORY = Path(os.path.abspath('')).resolve().parents[2]\n",
    "sys.path.extend([str(DATA_DIRECTORY)])\n",
    "\n",
    "print(f'Python {sys.version} on {sys.platform}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import utilities"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "NORMALIZATION = False\n",
    "STANDARDIZATION = False\n",
    "ONEHOTENCODING = False\n",
    "CONVLSTM = False\n",
    "\n",
    "COMPRESS = True\n",
    "COMPRESSION_SIZE = 128"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load and split dataset\n",
    "data, labels, fids, velocities, angles = utilities.load_dataset(DATA_DIRECTORY / 'data/data_adp.pkl')\n",
    "train_idx, test_idx, train_data, test_data = utilities.split_dataset(fids, labels, data)\n",
    "FILENAME = \"dataset\"\n",
    "\n",
    "# Clear\n",
    "del data\n",
    "\n",
    "# Preprocessing\n",
    "if NORMALIZATION:\n",
    "    FILENAME += \"_norm\"\n",
    "    utilities.normalize_data(train_data)\n",
    "    utilities.normalize_data(test_data)\n",
    "elif STANDARDIZATION:\n",
    "    FILENAME += \"_stand\"\n",
    "    utilities.standardize_data(train_data)\n",
    "    utilities.standardize_data(test_data)\n",
    "\n",
    "if COMPRESS:\n",
    "    FILENAME += f\"_comp{COMPRESSION_SIZE}\"\n",
    "    utilities.compress_data(train_data, COMPRESSION_SIZE)\n",
    "    utilities.compress_data(test_data, COMPRESSION_SIZE)\n",
    "\n",
    "if CONVLSTM:\n",
    "    FILENAME += \"_convLSTM\"\n",
    "    train_data = np.transpose(train_data, (0,2,1))\n",
    "    test_data = np.transpose(test_data, (0,2,1))\n",
    "\n",
    "    train_data = train_data.reshape((len(train_data),1,COMPRESSION_SIZE,6))\n",
    "    test_data = test_data.reshape((len(test_data),1,COMPRESSION_SIZE,6))\n",
    "\n",
    "train_labels = labels[train_idx]\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data,train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data,test_labels))\n",
    "\n",
    "# Generate mini batches\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = len(train_dataset)\n",
    "\n",
    "train_dataset_batch = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset_batch = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(len(train_dataset))\n",
    "for d, l in train_dataset.take(1):\n",
    "  print('data.shape: ', d.shape)\n",
    "  print('labels.shape: ', l.shape)\n",
    "\n",
    "print(len(train_dataset_batch))\n",
    "for d, l in train_dataset_batch.take(1):\n",
    "  print('data.shape: ', d.shape)\n",
    "  print('labels.shape: ', l.shape)\n",
    "\n",
    "LEN_SAMPLE = d.shape[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3x LSTM-Layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3 LSTM-Layer\n",
    "\n",
    "def model_builder(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  hp_units_lstm1 = hp.Int('lstm_units_1', min_value=16, max_value=256, step=32)\n",
    "  model.add(tf.keras.layers.LSTM(hp_units_lstm1, \n",
    "                                 recurrent_initializer='glorot_uniform', \n",
    "                                 recurrent_activation='sigmoid',\n",
    "                                 return_sequences=True, \n",
    "                                 stateful=True))\n",
    "  \n",
    "  if hp.Boolean('dropout_1'):\n",
    "    hp_dropout1_faktor = hp.Choice('dropout1_faktor', values=[0.2, 0.4, 0.6])\n",
    "    model.add(tf.keras.layers.Dropout(hp_dropout1_faktor))\n",
    "                        \n",
    "\n",
    "  hp_units_lstm2 = hp.Int('lstm_units_2', min_value=16, max_value=256, step=32)\n",
    "  model.add(tf.keras.layers.LSTM(hp_units_lstm2, \n",
    "                                 recurrent_initializer='glorot_uniform', \n",
    "                                 recurrent_activation='sigmoid',\n",
    "                                 return_sequences=True, \n",
    "                                 stateful=True))\n",
    "  \n",
    "  if hp.Boolean('dropout_2'):\n",
    "    hp_dropout2_faktor = hp.Choice('dropout2_faktor', values=[0.2, 0.4, 0.6])\n",
    "    model.add(tf.keras.layers.Dropout(hp_dropout2_faktor))\n",
    "\n",
    "  hp_units_lstm3 = hp.Int('lstm_units_3', min_value=16, max_value=256, step=32)\n",
    "  model.add(tf.keras.layers.LSTM(hp_units_lstm3, \n",
    "                                 recurrent_initializer='glorot_uniform', \n",
    "                                 recurrent_activation='sigmoid',\n",
    "                                 return_sequences=False, \n",
    "                                 stateful=True))\n",
    "  \n",
    "  if hp.Boolean('dropout_3'):\n",
    "    hp_dropout3_faktor = hp.Choice('dropout3_faktor', values=[0.2, 0.4, 0.6])\n",
    "    model.add(tf.keras.layers.Dropout(hp_dropout3_faktor))\n",
    "    \n",
    "\n",
    "  hp_units_postprocession_dense = hp.Int('postprocession_dense_units', min_value=16, max_value=128, step=16)\n",
    "  hp_activation_Dense = hp.Choose('activiation_dense', ['relu', 'tanh', 'sigmoid'])\n",
    "  model.add(tf.keras.layers.Dense(hp_units_postprocession_dense, \n",
    "                                  activation=hp_activation_Dense))\n",
    "\n",
    "\n",
    "  hp_dropout_faktor = hp.Choice('dropout', values=[0.2, 0.4, 0.6])\n",
    "  model.add(tf.keras.layers.Dropout(hp_dropout_faktor))\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "  # Tune Learning-Rate\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')])\n",
    "\n",
    "  return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2x LSTM-Layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2 LSTM-Layer\n",
    "\n",
    "def model_builder(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  hp_units_preprocession_dense = hp.Int('preprocession_dense_units', min_value=64, max_value=128, step=32)\n",
    "  model.add(tf.keras.layers.Dense(hp_units_preprocession_dense, \n",
    "                                  activation='relu'))\n",
    "  if hp.Boolean('lstm_1'):\n",
    "    hp_units_lstm1 = hp.Int('lstm_units_1', min_value=64, max_value=128, step=32)\n",
    "    model.add(tf.keras.layers.LSTM(hp_units_lstm1, \n",
    "                                    recurrent_initializer='glorot_uniform', \n",
    "                                    recurrent_activation='sigmoid',\n",
    "                                    return_sequences=True, \n",
    "                                    stateful=True))\n",
    "    \n",
    "\n",
    "\n",
    "  hp_units_lstm2 = hp.Int('lstm_units_2', min_value=64, max_value=128, step=32)\n",
    "  model.add(tf.keras.layers.LSTM(hp_units_lstm2, \n",
    "                                    recurrent_initializer='glorot_uniform', \n",
    "                                    recurrent_activation='sigmoid',\n",
    "                                    return_sequences=False, \n",
    "                                    stateful=True))\n",
    "\n",
    "  if hp.Boolean('dropout_1'):\n",
    "    hp_dropout1_faktor = hp.Choice('dropout1_faktor', values=[0.2, 0.4, 0.6])\n",
    "    model.add(tf.keras.layers.Dropout(hp_dropout1_faktor))\n",
    "                            \n",
    "\n",
    "  hp_units_postprocession_dense = hp.Int('postprocession_dense_units', min_value=32, max_value=64, step=32)\n",
    "  hp_activation_Dense = hp.Choice('activiation_dense', ['relu', 'tanh', 'sigmoid'])\n",
    "  model.add(tf.keras.layers.Dense(hp_units_postprocession_dense, \n",
    "                                  activation=hp_activation_Dense))\n",
    "\n",
    "\n",
    "  hp_dropout_faktor = hp.Choice('dropout', values=[0.2, 0.4, 0.6])\n",
    "  model.add(tf.keras.layers.Dropout(hp_dropout_faktor))\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')])\n",
    "\n",
    "  return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1x LSTM-Layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1 LSTM-Layer\n",
    "\n",
    "def model_builder(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  hp_units_preprocession_dense = hp.Int('preprocession_dense_units', min_value=16, max_value=256, step=32)\n",
    "  model.add(tf.keras.layers.Dense(hp_units_preprocession_dense, \n",
    "                                  activation='relu'))\n",
    "\n",
    "  hp_units_lstm1 = hp.Int('lstm_units_1', min_value=64, max_value=256, step=32)\n",
    "  model.add(tf.keras.layers.LSTM(hp_units_lstm1, \n",
    "                                 recurrent_initializer='glorot_uniform', \n",
    "                                 recurrent_activation='sigmoid',\n",
    "                                 return_sequences=False, \n",
    "                                 stateful=True))\n",
    "  \n",
    "  if hp.Boolean('dropout_1'):\n",
    "    hp_dropout1_faktor = hp.Choice('dropout1_faktor', values=[0.2, 0.4, 0.6])\n",
    "    model.add(tf.keras.layers.Dropout(hp_dropout1_faktor))\n",
    "                            \n",
    "  hp_units_postprocession_dense = hp.Int('postprocession_dense_units', min_value=64, max_value=256, step=32)\n",
    "  hp_activation_Dense = hp.Choice('activiation_dense', ['relu', 'tanh', 'sigmoid'])\n",
    "  model.add(tf.keras.layers.Dense(hp_units_postprocession_dense, \n",
    "                                  activation=hp_activation_Dense))\n",
    "\n",
    "  hp_dropout_faktor = hp.Choice('dropout', values=[0.2, 0.4, 0.6])\n",
    "  model.add(tf.keras.layers.Dropout(hp_dropout_faktor))\n",
    "\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')])\n",
    "\n",
    "  return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Choose hyper param search algorithm\n",
    "HYPERBAND_TUNER = True\n",
    "RANDOM_TUNER = False\n",
    "BAYESIAN_TUNER = False\n",
    "EPOCHS = 10\n",
    "\n",
    "SEARCH = \"Meta\"\n",
    "TRIAL = 10\n",
    "PROJECT_NAME = f\"lstm_{BATCH_SIZE}_{FILENAME}_{SEARCH}_{TRIAL}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use Hyperband first -> searches for good settings over 2 Epochs and train the best ones again, \n",
    "# chooses parameters also just randomly\n",
    "\n",
    "if HYPERBAND_TUNER:\n",
    "    DIRECTORY = \"hyper-search\"\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        model_builder,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=10,\n",
    "        factor=2,\n",
    "        directory=DIRECTORY,\n",
    "        project_name=PROJECT_NAME\n",
    "    )\n",
    "\n",
    "# Uses random combination of parameters (against Grid-Search, which takes every single combination)\n",
    "\n",
    "if RANDOM_TUNER:\n",
    "    DIRECTORY = \"random-search\"\n",
    "\n",
    "    tuner = kt.RandomSearch(\n",
    "        hypermodel=model_builder,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=3,\n",
    "        executions_per_trial=2,\n",
    "        overwrite=True,\n",
    "        directory=DIRECTORY,\n",
    "        project_name=PROJECT_NAME\n",
    "    )\n",
    "\n",
    "# Just takes the first few parameters at random, after that takes the best performing ones to continue\n",
    "\n",
    "if BAYESIAN_TUNER:\n",
    "    DIRECTORY = \"bayesian-search\"\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        hypermodel=model_builder,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=40,\n",
    "        directory=DIRECTORY,\n",
    "        project_name=PROJECT_NAME,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=DIRECTORY + '/tb_logs_' + PROJECT_NAME, histogram_freq=1)\n",
    "tuner.search(train_dataset_batch,\n",
    "             epochs=EPOCHS,\n",
    "             callbacks=[stop_early, tensorboard_callback],\n",
    "             validation_data=test_dataset_batch)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=5)[0]\n",
    "print(best_hps.values)\n",
    "\n",
    "tuner.results_summary(num_trials=2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_model = tuner.get_best_models(num_models=1)\n",
    "# Model evaluate\n",
    "test_loss, test_acc = best_model.evaluate(test_dataset_batch)\n",
    "print('Test accuracy:', test_acc)\n",
    "best_model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "best_model.save(PROJECT_DIRECTORY / 'models/2lstm128')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Choose best model\n",
    "best_hps = tuner.get_best_hyperparameters(2)\n",
    "best_model = model_builder(best_hps[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "tf.keras.backend.clear_session()",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
